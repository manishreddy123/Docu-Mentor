# ğŸ“„ Docu-Mentor â€” Agentic RAG PDF Question Answering System

Docu-Mentor is a powerful, modular, and explainable **document QA system** that uses:

- ğŸ§  Multi-agent orchestration  
- ğŸ” Retrieval-Augmented Generation (RAG)  
- ğŸ¤– LLMs for answer synthesis  
- ğŸ—‚ Page-level referencing with chunk highlighting  
- âœ… Semantic deduplication, hybrid reranking, and memory-free stateless context

---

## ğŸš€ Features

- Upload PDFs, DOCX, TXT, CSV, or MD
- Chunking, Embedding & Vector Store caching
- Query rewriting via intent classification
- Hybrid semantic retrieval: FAISS, ChromaDB, HNSW & ColBERT
- CrossEncoder + LLM-based reranking
- Markdown-based Streamlit chat UI with PDF preview
- ğŸ”— Page number references beneath each answer

---

## ğŸ§  What Each Agent Does

| Agent | Description |
|-------|-------------|
| **IngestionAgent** | Loads and parses documents, generates chunks, computes embeddings, saves to vector store |
| **EmbeddingAgent** | Deduplicates chunks, encodes using transformer models, builds HNSW index and Chroma collection |
| **RetrievalAgent** | Retrieves top-k chunks via HNSW, FAISS, Chroma + hybrid ColBERT scoring |
| **QueryRewriteAgent** | Classifies query intent (statistical, causal, factual...) and rewrites query using few-shot prompting |
| **RerankerAgent** | Reranks retrieved chunks using CrossEncoder and optionally LLM-based ReAct scoring |
| **LLMResponseAgent** | Builds final prompt and streams answer using OpenRouter models (GPT-4, Claude, Grok) with fallback |
| **PromptFormatterAgent** | Converts chunks into formatted prompts (QA, comparison, summarization, extraction) |
| **ColBERTRetrievalAgent** | Performs token-level semantic scoring using ColBERT-style late interaction retrieval |

---

## ğŸ› ï¸ Project Setup

### 1. ğŸ“¦ Clone the Repo

```bash
git clone https://github.com/yourname/documind-lite.git
cd documind-lite
```

### 2. ğŸ Create Virtual Environment

```bash
python -m venv .venv
# Activate:
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
```

### 3. ğŸ“š Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. ğŸ—‚ï¸ Create Required Folders

Create the following directories if they donâ€™t exist:

```bash
mkdir data models vector_store
```

These folders are used to store:
- `data/` â€” uploaded PDFs and documents
- `models/` â€” downloaded SentenceTransformer / ColBERT models
- `vector_store/` â€” persistent FAISS, ChromaDB, and memory storage

> âœ… You must ensure these folders are present before running the app.

---

### 5. ğŸ”‘ Setup Environment Variables

Create a `.env` file at the root with your API keys and preferences:

```env
OPENROUTER_API_KEY=your_openrouter_key
DEFAULT_MODEL=openrouter/openai/gpt-4
MODEL_1=openrouter/openai/gpt-4
MODEL_2=openrouter/anthropic/claude-3-opus
MODEL_3=openrouter/grok-1
TEMPERATURE=0.4
```

---

### 6. ğŸ“‚ Setup Poppler (for PDF preview rendering)

#### ğŸªŸ Windows

- Download from: https://github.com/oschwartz10612/poppler-windows/releases/
- Extract to:  
  ```
  documind-main/poppler/Library/bin
  ```

#### ğŸ§ Linux / macOS

```bash
sudo apt install poppler-utils
```

---

### 7. ğŸš€ Launch the App

```bash
streamlit run app.py
```

---

## ğŸ“ Directory Structure

```
docu-mentor/
â”œâ”€â”€ app.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ viewer_component.py
â”œâ”€â”€ upload_modal.py
â”œâ”€â”€ session_manager.py
â”œâ”€â”€ streaming_response.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ingestion_agent.py
â”‚   â”œâ”€â”€ embedding_agent.py
â”‚   â”œâ”€â”€ retrieval_agent.py
â”‚   â”œâ”€â”€ query_rewrite_agent.py
â”‚   â”œâ”€â”€ reranker_agent.py
â”‚   â”œâ”€â”€ llm_response_agent.py
â”‚   â”œâ”€â”€ prompt_formatter_agent.py
â”‚   â””â”€â”€ colbert_retrieval_agent.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent_manager.py
â”‚   â”œâ”€â”€ config_manager.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ hnswlib_search.py
â”‚   â”œâ”€â”€ mcp.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ model_manager.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ page_utils.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ [uploaded documents + vector store data]
â”œâ”€â”€ models/
â”‚   â””â”€â”€ [HF-downloaded SentenceTransformer models]
â””â”€â”€ vector_store/
    â”œâ”€â”€ faiss_store.pkl
    â”œâ”€â”€ chroma/
    â”œâ”€â”€ memory/
    â””â”€â”€ doc_cache/
```

---

## ğŸ§  Architecture Overview

```mermaid
graph TD
    A[Upload PDF] --> B[Parse and Chunk]
    B --> C[Embed Chunks]
    C --> D[Deduplicate Chunks]
    D --> E[Classify Query Intent]
    E --> F[Rewrite Query]
    F --> G[Semantic Search: FAISS, Chroma, HNSW]
    G --> H[Rerank with ColBERT and CrossEncoder]
    H --> I[Select Top-k Chunks]
    I --> J[Generate Answer using LLM]
    J --> K[Extract Source References]
    K --> L[Display Answer and Page Numbers]

```

---

## ğŸ”„ Full Process Breakdown

1. **ğŸ“¤ Upload PDF**  
   `app.py` handles file upload and stores it in `/data`

2. **ğŸ“„ Parse & Chunk**  
   `document_loader.py` extracts text based on format and chunks it

3. **ğŸ”¡ Embedding**  
   `embedding_agent.py` generates embeddings via BGE or MiniLM

4. **ğŸ§¼ Deduplication**  
   Chunks with >92% similarity are dropped using cosine distance

5. **âœï¸ Query Rewrite**  
   `query_rewrite_agent.py` classifies query type and rewrites it for optimal retrieval

6. **ğŸ” Retrieval**  
   `retrieval_agent.py`:
   - Tries HNSW first
   - Falls back to FAISS + Chroma
   - Applies ColBERT-based scoring

7. **âš–ï¸ Reranking**  
   `reranker_agent.py`:
   - Uses `ms-marco-MiniLM` CrossEncoder
   - Optionally reranks via LLM using ReAct-style reasoning

8. **ğŸ¤– LLM Answering**  
   `llm_response_agent.py` builds a structured prompt using:
   - Retrieved chunks
   - Model fallback and retry logic

9. **ğŸ“„ Reference Extraction**  
   `"source": "filename.pdf p. 3"` from chunks is parsed

10. **ğŸ’¬ Final Output**  
    - Assistant response shown
    - Page numbers displayed as:
      ```
      ğŸ”— References: Page 3, Page 7
      ```

---

## âœ… Credits

Built with:

- [Streamlit](https://streamlit.io/)
- [LangChain](https://www.langchain.com/)
- [Sentence-Transformers](https://www.sbert.net/)
- [ChromaDB](https://www.trychroma.com/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [HNSWlib](https://github.com/nmslib/hnswlib)
- [POPPLER](https://github.com/oschwartz10612/poppler-windows)

---

